# CKS 이론

## 문제 유형

1. network policy

- deny all 하는 문제가 있으며, development 네임스페이스의 모든 파드에 대해서 진행, egress

- 패캠에서는 label 기반으로 통제하는 것을 배웟는데 문제는 시험에는 label이 없음

- 그래서 메타데이터 상 name 밑에 namespace를 두고 생성 > 실제로 테스트 해보니 egress 잘먹혀서 이렇게 일단 고

2. securityContainer

- Dockerfile (2개 수정)

- 베이스 이미지 ubunta latest > ubuntu 22.04 문제에 나옴

- 문제는 USER ROOT를 사용하는데 이건 문제가 있는데 어떻게 고쳐야하는건지 모르겠음 ? 사용자를 제시안해주는것 같은데..

- Deployment yaml (2개 수정)

- 65535 uid 사용하라고 되어있고 2개 찾으라는데 못찾음

3. gVisor container runtime

- 해당 런타임클래스 생성하고 파드에 적용시키면 끝

4. Audit 로그

- 기존에 하던대로 config 넣고 진행

5. CIS-benchmark

- 이미 벤치마크 된거에 대해서 수정하라고 함 (문제에 다 주어짐)

- kubelet > 반드시 수정한 내용을 적용하기 위해서 systemctl restart kubelet 할것

- api-server 수정

6. secret 생성

- 기존에 생성된 시크릿에 대한 base64 -d 디코딩 작업

- 새로운 secret 생성

7. service account

- role 생성

- rolebinding 생성 (참고로 kubernetes 공식문서에 rolebinding cli 예시가 clusterrole로 나와있어서 주의요함, role로 바꿔적어야함)

- pod에 있는 sa를 알아서 찾은 다음 그 sa에 rolebinding 진행할 것

8. network policy

- 생성하는거 나옴

9. trivy 취약점

- trivy 스캔후 high, critical 파드 제거 (순간 잘못지워서........... 그냥 똑같은 파드 하나 내가 임의로 만들어버림..)

10. sysdig / falco

- 난 sysdig 이용해서 사용했는데 container id를 crictl로 확인해서 분명 명령어 쳤으나 안되서 container name으로 변경

- proc.name / uid 혹은 username / timestamp를 주어지는데 값이 이상하긴 했음..

11. app armor

- parser로 일단 policy 등록하고, yaml 만들어서 등록함.

- aa-status로 잘되는지 확인

12. Container Immutability

- 일단 readOnlyRootFilesystem랑 특정 uid 사용, allowPrivilegeEscalation 적용에 대해서 하라고 함.

13. audit log

- 기존에 있는 audit로그를 적용하면서 etcd랑 apiserver 변경하는것같았음

14. etcd / api-server tls 적용

- 적용될 tls 내용을 주니까 그걸 사용하면될 것 같고

etcd의 경우는 -- cipher-suite 이런걸로 쓰는것 같길래 이걸로 옵션적용. etcd가 잘뜨는지 확인햇음.

나머지 2개는 잘 기억안남

추후 참고

[https://jonathan18186.medium.com/certified-kubernetes-security-specialist-cks-preparation-part-5-microservice-vulnerabilities-b5284a46d6bc](https://jonathan18186.medium.com/certified-kubernetes-security-specialist-cks-preparation-part-5-microservice-vulnerabilities-b5284a46d6bc)

[https://velog.io/@comeonyo/CKS-%EC%8B%9C%ED%97%98-%EB%B2%94%EC%9C%84-%EC%A0%95%EB%A6%AC](https://velog.io/@comeonyo/CKS-%EC%8B%9C%ED%97%98-%EB%B2%94%EC%9C%84-%EC%A0%95%EB%A6%AC)

참고

[CKS 문제 공유](https://devops-james.tistory.com/m/233)

# 이론

인증서파일 위치

/etc/kubernetes/pki에 위치한 파일

Etcd 인증서는

/etc/kubernetes/pki/etcd

스케줄러 위치 api

/etc/kubernetes/scheduler.conf

컨트롤러 위치 api

/etc/kubernetes/controller-manager.conf

kubelet 위치 api

/etc/kubernetes/kubelet.conf 에서 확인가능

/var/lib/kubelet/pki/kubelet-client-current.pem

kubelet server cert

워커노드에  /var/lib/kubelet/pki/kubelet-client-current.pem

container 툴

Docker: Container Runtime + Tool for managing containers and images
Containerd: Container Runtime
Crictl: CLI for CRI-compatible Container Runtimes

   /etc/crictl.yaml
Podman: Tool for managing containers and images

컨테이너를 생성하면 커널에서 조회가 가능하다.

컨테이너 1의 pid와 컨테이너 2의 Pid는 구분이 되는데 하나의 프로세스에서 올라갈 수 있는 방법

```jsx
docker run --name c1 -d nginx:alpine sh -c "sleep 999d"
docker run --name c2 --pid=container:c1 -d nginx:alpine sh -c "sleep 999d"

docker exec c2 ps aux
docker exec c1 ps aux
```

## 문제

There are existing *Pods* in *Namespace* `app` .

We need a new default-deny *NetworkPolicy* named `deny-out` for all outgoing traffic from *Namespace* `app` .

It should still allow DNS traffic on port `53` TCP and UDP.

Namespace app 에 기존 Pod가 있습니다. Namespace app 에서 나가는 모든 트래픽에 대해 거부라는 이름의 새로운 기본 거부 NetworkPolicy가 필요합니다. 포트 53 TCP 및 UDP에서 DNS 트래픽을 계속 허용해야 합니다.

```jsx
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-out
  namespace: app
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - ports:
    - port: 53
      protocol: TCP
    - port: 53
      protocol: UDP
```

## 문제

There are existing *Pods* in *Namespace* `space1` and `space2` .We need a new *NetworkPolicy* named `np` that restricts all *Pods* in *Namespace* `space1` to only have outgoing traffic to *Pods* in *Namespace* `space2` . Incoming traffic not affected.We also need a new *NetworkPolicy* named `np` that restricts all *Pods* in *Namespace* `space2` to only have incoming traffic from *Pods* in *Namespace* `space1` . Outgoing traffic not affected.The *NetworkPolicies* should still allow outgoing DNS traffic on port `53` TCP and UDP.

1.`k get ns --show-labels`

2.

```jsx
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: np
  namespace: space1
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
     - namespaceSelector:
        matchLabels:
         kubernetes.io/metadata.name: space2
  - ports:
    - port: 53
      protocol: TCP
    - port: 53
      protocol: UDP
```

3.

```jsx
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: np
  namespace: space2
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
   - from:
     - namespaceSelector:
        matchLabels:
         kubernetes.io/metadata.name: space1
```

---

TIP 

**What Is the Difference Between Kubectl-Proxy and Kubectl Port-Forward?**

- Kubectl-proxy creates a proxy server between your local computer and the Kubernetes API server. This means that any requests made to the Kubernetes API server by the client are forwarded through the proxy.
The main use case of **"**kubectl proxy**"** is to access the Kubernetes API server.
On the other hand, the **"**kubectl port-forward**"** command creates a tunnel from a local port on your machine to the target port on the Pod. This is especially useful when you want to access a specific Pod directly, like when debugging an application for example.
In summary, **"**kubectl proxy**"** is more suitable for general cluster access, while **"**kubectl port-forward**"** is better for targeting specific Pods.
    
    Kubectl-proxy는 로컬 컴퓨터와 Kubernetes API 서버 사이에 프록시 서버를 생성합니다. 이는 클라이언트가 Kubernetes API 서버에 보낸 모든 요청이 프록시를 통해 전달됨을 의미합니다.
    
     "kubectl 프록시"의 주요 사용 사례는 Kubernetes API 서버에 액세스하는 것입니다. 반면에 'kubectl port-forward' 명령은 머신의 로컬 포트에서 포드의 대상 포트까지 터널을 생성합니다. 
    
    이는 예를 들어 애플리케이션을 디버깅할 때와 같이 특정 Pod에 직접 액세스하려는 경우 특히 유용합니다. 
    
    요약하면, "kubectl Proxy"는 일반 클러스터 액세스에 더 적합하고, "kubectl port-forward"는 특정 포드를 대상으로 하는 데 더 적합합니다.
    

port-forward는 Pod와 통신을 위해

proxy는 kube-apiserver와 통신을 위해

사용 예제 -kubectl port-forward 

[https://kodekloud.com/blog/port-forwarding-kubernetes/](https://kodekloud.com/blog/port-forwarding-kubernetes/)

- 도메인이 없는 경우 curl 로 지정해줘서 날려보내기
- curl [https://domain](https://domain):port/path  -k  —resolve domain:IP
    
    

## 문제

There are two existing *Deployments* in *Namespace* `world` which should be made accessible via an *Ingress*.
First: create ClusterIP *Services* for both *Deployments* for port `80` . The *Services* should have the same name as the *Deployments*.

```jsx
kubectl expose deploy -n word  asia --port 80 --type ClusterIP

```

**추가**

The Nginx Ingress Controller has been installed.

Create a new *Ingress* resource called `world` for domain name `world.universe.mine` . The domain points to the K8s Node IP via `/etc/hosts` .

The *Ingress* resource should have two routes pointing to the existing *Services*:

`http://world.universe.mine:30080/europe/`

and

`http://world.universe.mine:30080/asia/`

```jsx
kubectl get ingressclass -A 

-----------
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: world
  namespace: world
spec:
  ingressClassName: nginx # k get ingressclass
  rules:
  - host: "world.universe.mine"
    http:
      paths:
      - path: /europe
        pathType: Prefix
        backend:
          service:
            name: europe
            port:
              number: 80
      - path: /asia
        pathType: Prefix
        backend:
          service:
            name: asia
            port:
              number: 80
```

## 문제

Make sure to have solved the previous Scenario [Ingress Create](https://killercoda.com/killer-shell-cks/scenario/ingress-create).

The Nginx Ingress Controller has been installed and an *Ingress* resource configured in *Namespace* `world` .

You can reach the application using

`curl http://world.universe.mine:30080/europe`

Generate a new TLS certificate using:

`openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout cert.key -out cert.crt -subj "/CN=world.universe.mine/O=world.universe.mine"`

Configure the *Ingress* to use the new certificate, so that you can call

`curl -kv https://world.universe.mine:30443/europe`

The curl verbose output should show the new certificate being used instead of the default *Ingress* one.

```jsx
openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout cert.key -out cert.crt -subj "/CN=world.universe.mine/O=world.universe.mine"
kubectl create secret tls tls -n world  --cert=cert.crt --key=cert.key

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: world
  namespace: world
spec:
  ingressClassName: nginx
  rules:
  - host: world.universe.mine
    http:
      paths:
      - backend:
          service:
            name: europe
            port:
              number: 80
        path: /europe
        pathType: Prefix
      - backend:
          service:
            name: asia
            port:
              number: 80
        path: /asia
        pathType: Prefix
  tls:
  - hosts:
    - world.universe.mine
    secretName: tls

```

## 문제

Cloud providers can have Metadata Servers which expose critical information, for example [GCP](https://cloud.google.com/compute/docs/metadata/overview) or [AWS](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html).For this task we assume that there is a Metadata Server at `1.1.1.1` .
You can test connection to that IP using `nc -v 1.1.1.1 53` .
Create a *NetworkPolicy* named `metadata-server` In *Namespace* `default` which restricts all egress traffic to that IP.
The *NetworkPolicy* should only affect *Pods* with label `trust=nope` .

```jsx
k get pod -L trust

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: metadata-server
  namespace: default
spec:
  podSelector:
    matchLabels:
      trust: nope
  policyTypes:
  - Egress
  egress:
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
          - 1.1.1.1/32
```

K8S 벤치마크 툴  kube-bench

[CIS_Kubernetes_Benchmark_v1.6.0.pdf](../CIS_Kubernetes_Benchmark_v1.6.0.pdf)

```jsx
# how to run
https://github.com/aquasecurity/kube-bench/blob/main/docs/running.md

# run on master
docker run --pid=host -v /etc:/etc:ro -v /var:/var:ro -t aquasec/kube-bench:latest run --targets=master --version 1.22

# run on worker
docker run --pid=host -v /etc:/etc:ro -v /var:/var:ro -t aquasec/kube-bench:latest run --targets=node --version 1.22
```

## 문제

Use `kube-bench` to ensure 1.2.20 has status PASS.

```jsx
# see all
kube-bench run --targets master

# or just see the one
kube-bench run --targets master --check 1.2.20
```

![Untitled](../Images/cks1.png)

```jsx
vi /etc/kubernetes/manifests/kube-apiserver.yaml
...
containers:
  - command:
    - kube-apiserver
    - --profiling=false
```

원하는 release 다운로드 

주소 : https://github.com/kubernetes/kubernetes

tags 를 눌러 원하는 릴리즈를 선택  

additional binarty downloads are linked in the  changelog/changelog-1.19.md  눌러 접근

서버 바이너리에서 > wget을 하여 가져온다.

```jsx
sha512sum  [파일이름] > [new name]

```

## 문제

Download the `kubelet` binary in the same version as the installed one.
`wget https://dl.k8s.io/vX.Y.Z/kubernetes-server-linux-amd64.tar.gz`
Compare their sha hashes and answer with:
`echo SAME > /answer`
`echo DIFFERENT > /answer`

```jsx
VERSION = $(kubelet --version |cut -d ' ' -f2 )
wget https://dl.k8s.io/$VERSION/kubernetes-server-linux-amd64.tar.gz
tar xzf kubernetes-server-linux-amd64.tar.gz

--
whereis kubelet
sha512sum /usr/bin/kubelet
sha512sum kubernetes/server/bin/kubelet
```

## 문제 NP- metaserver in CSP

Cloud providers can have Metadata Servers which expose critical information, for example [GCP](https://cloud.google.com/compute/docs/metadata/overview) or [AWS](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html).

For this task we assume that there is a Metadata Server at `1.1.1.1` .

You can test connection to that IP using `nc -v 1.1.1.1 53` .

Create a *NetworkPolicy* named `metadata-server` In *Namespace* `default` which restricts all egress traffic to that IP.

The *NetworkPolicy* should only affect *Pods* with label `trust=nope` .

```jsx
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: metadata-server
  namespace: default
spec:
  podSelector:
    matchLabels:
      trust: nope
  policyTypes:
  - Egress
  egress:
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
          - 1.1.1.1/32
```